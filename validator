import logging
import json
import sys
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions

# Setting up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Global Spark session setup
sc = SparkContext()
glueContext = GlueContext(sc)
spark = SparkSession.builder.appName("DataValidation").getOrCreate()


# Validation functions
def validate_not_null(df, col_name):
    return df.filter(f"{col_name} IS NULL")


def validate_is_numeric(df, col_name):
    return df.filter(~df[col_name].rlike("^-?\\d+(\\.\\d+)?$"))


# Global validations configuration
VALIDATIONS_CONFIG = {
    "not_null": {
        "function": validate_not_null,
        "message": "null values"
    },
    "is_numeric": {
        "function": validate_is_numeric,
        "message": "non-numeric values"
    }
}


def log_failures(sample_failing_rows, column, validation_type, message, output_path):
    if sample_failing_rows:
        logging.error(f"Validation {validation_type} failed for {column}. Found {len(sample_failing_rows)} {message}.")
        output_df = spark.createDataFrame(sample_failing_rows)
        output_df.write.mode("overwrite").csv(f"{output_path}/{column}_{validation_type}")
    else:
        logging.info(f"Validation {validation_type} passed for {column}. No {message} found.")


def run_validations(s3_path, output_path, column_validations_config):
    # 1. Read the CSV data from the provided S3 path into a DataFrame.
    #    - `header=True` means the first row of the CSV file contains column headers.
    #    - `inferSchema=True` will automatically detect the data type of each column.
    df = spark.read.csv(s3_path, header=True, inferSchema=True)

    # 2. Iterate over the columns and their associated validations from the provided configuration.
    for column, column_validations in column_validations_config.items():

        # 3. For each column, iterate over its validations.
        for validation in column_validations:
            # 4. Fetch the appropriate validation function from the global VALIDATIONS_CONFIG dictionary.
            validation_function = VALIDATIONS_CONFIG[validation]["function"]

            # 5. Apply the validation function on the DataFrame to get rows that fail the validation.
            failing_df = validation_function(df, column)

            # 6. Collect a sample of up to 10 failing rows. This is to provide a snapshot of the issues without overwhelming the logs.
            sample_failing_rows = failing_df.limit(10).collect()

            # 7. Fetch the associated message for the current validation from the global VALIDATIONS_CONFIG dictionary.
            message = VALIDATIONS_CONFIG[validation]["message"]

            # 8. Log the failures using the log_failures function.
            #    This will log the issue and save the sample failing rows to the specified output path.
            log_failures(sample_failing_rows, column, validation, message, output_path)


if __name__ == "__main__":
    try:
        args = getResolvedOptions(sys.argv, ['s3_path', 'validations', 'output_path'])
        validations_config = json.loads(args['validations'])

        run_validations(args['s3_path'], args['output_path'], validations_config)
    except json.JSONDecodeError:
        logging.error("Error parsing 'validations' argument. Ensure it's valid JSON.")
    except Exception as e:
        logging.exception("An error occurred during execution.")
